{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['multi_lr','decision_tree','multi_NB','BNB'] # <-- add yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the metric dataframe from the above model names\n",
    "df_results = pd.DataFrame()\n",
    "\n",
    "for model_name in model_names:\n",
    "    path = f'../data/pickles/df_results_{model_name}.pickle'\n",
    "    with open(path, 'rb') as data:\n",
    "        df = pickle.load(data)\n",
    "    df_results = df_results.append(df)\n",
    "    \n",
    "df_results = df_results.reset_index().drop('index', axis=1)\n",
    "\n",
    "metrics = {\n",
    "    'accuracy': 'Accuracy',\n",
    "    'precision_macro': 'Precision (macro)',\n",
    "    'recall_macro': 'Recall (macro)',    \n",
    "    'f1_macro': 'F1 (macro)',\n",
    "    'precision': 'Precision (micro)',\n",
    "    'recall': 'Recall (micro)',    \n",
    "    'f1': 'F1 (micro)',\n",
    "}\n",
    "\n",
    "for metric in metrics:\n",
    "    df_results[metrics[metric]] = df_results[f'mean_test_{metric}'].map('{:,.4f}'.format).astype(str) \\\n",
    "    + df_results[f'std_test_{metric}'].map(' ±{:,.3f}'.format).astype(str)\n",
    "    \n",
    "df_results.sort_values('mean_test_f1_macro', inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation comparison\n",
    "\n",
    "Cross-validation results on the training set for selected metrics, feature sets and implemented methods.\n",
    "The following results show the mean validation and standard deviation value for each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision (macro)</th>\n",
       "      <th>Recall (macro)</th>\n",
       "      <th>F1 (macro)</th>\n",
       "      <th>Precision (micro)</th>\n",
       "      <th>Recall (micro)</th>\n",
       "      <th>F1 (micro)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multi_lr</td>\n",
       "      <td>0.7728 ±0.006</td>\n",
       "      <td>0.6284 ±0.022</td>\n",
       "      <td>0.7092 ±0.018</td>\n",
       "      <td>0.6557 ±0.019</td>\n",
       "      <td>0.6552 ±0.009</td>\n",
       "      <td>0.7277 ±0.005</td>\n",
       "      <td>0.6895 ±0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>multi_NB</td>\n",
       "      <td>0.6667 ±0.010</td>\n",
       "      <td>0.5512 ±0.015</td>\n",
       "      <td>0.7105 ±0.017</td>\n",
       "      <td>0.5848 ±0.016</td>\n",
       "      <td>0.6667 ±0.010</td>\n",
       "      <td>0.6667 ±0.010</td>\n",
       "      <td>0.6667 ±0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BNB</td>\n",
       "      <td>0.7081 ±0.005</td>\n",
       "      <td>0.5682 ±0.023</td>\n",
       "      <td>0.5658 ±0.025</td>\n",
       "      <td>0.5465 ±0.025</td>\n",
       "      <td>0.6017 ±0.007</td>\n",
       "      <td>0.6479 ±0.009</td>\n",
       "      <td>0.6240 ±0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.6573 ±0.003</td>\n",
       "      <td>0.4310 ±0.005</td>\n",
       "      <td>0.5611 ±0.010</td>\n",
       "      <td>0.4815 ±0.002</td>\n",
       "      <td>0.5318 ±0.006</td>\n",
       "      <td>0.6242 ±0.009</td>\n",
       "      <td>0.5743 ±0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model       Accuracy Precision (macro) Recall (macro)  \\\n",
       "0       multi_lr  0.7728 ±0.006     0.6284 ±0.022  0.7092 ±0.018   \n",
       "2       multi_NB  0.6667 ±0.010     0.5512 ±0.015  0.7105 ±0.017   \n",
       "3            BNB  0.7081 ±0.005     0.5682 ±0.023  0.5658 ±0.025   \n",
       "1  decision_tree  0.6573 ±0.003     0.4310 ±0.005  0.5611 ±0.010   \n",
       "\n",
       "      F1 (macro) Precision (micro) Recall (micro)     F1 (micro)  \n",
       "0  0.6557 ±0.019     0.6552 ±0.009  0.7277 ±0.005  0.6895 ±0.006  \n",
       "2  0.5848 ±0.016     0.6667 ±0.010  0.6667 ±0.010  0.6667 ±0.010  \n",
       "3  0.5465 ±0.025     0.6017 ±0.007  0.6479 ±0.009  0.6240 ±0.007  \n",
       "1  0.4815 ±0.002     0.5318 ±0.006  0.6242 ±0.009  0.5743 ±0.006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Precision, Recall and F1 scores exclude the 'IRRELEVANT' class\n"
     ]
    }
   ],
   "source": [
    "display(df_results[['model', *metrics.values()]])\n",
    "print('Note: Precision, Recall and F1 scores exclude the \\'IRRELEVANT\\' class')\n",
    "# print(df_results[['model', 'Accuracy', 'Precision (macro)', 'Recall (macro)', 'F1 (macro)']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final results\n",
    "\n",
    "Final results for each class calculated on the whole test set using the final selected method with its hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = 'multi_lr' # <-- we can change this to the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load the selected final model\n",
    "path = f'../data/pickles/best_{selected_model}.pickle'\n",
    "with open(path, 'rb') as data:\n",
    "    final_model = pickle.load(data)\n",
    "    \n",
    "# Load the test set\n",
    "df_test = pd.read_csv('../data/test.csv', index_col=0).reset_index()\n",
    "X_test = df_test.article_words.values\n",
    "y_test = df_test.topic.values\n",
    "\n",
    "# Load labels\n",
    "labels = sorted(list(set(y_test)))\n",
    "labels.remove('IRRELEVANT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results using the 'multi_lr' model:\n",
      "\n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "      ARTS CULTURE ENTERTAINMENT       0.38      1.00      0.55         3\n",
      "BIOGRAPHIES PERSONALITIES PEOPLE       0.78      0.47      0.58        15\n",
      "                         DEFENCE       0.75      0.92      0.83        13\n",
      "                DOMESTIC MARKETS       0.40      1.00      0.57         2\n",
      "                   FOREX MARKETS       0.57      0.77      0.65        48\n",
      "                          HEALTH       0.69      0.79      0.73        14\n",
      "                   MONEY MARKETS       0.62      0.59      0.61        69\n",
      "          SCIENCE AND TECHNOLOGY       0.33      0.33      0.33         3\n",
      "                  SHARE LISTINGS       0.60      0.86      0.71         7\n",
      "                          SPORTS       0.95      0.98      0.97        60\n",
      "\n",
      "                       micro avg       0.69      0.76      0.72       234\n",
      "                       macro avg       0.61      0.77      0.65       234\n",
      "                    weighted avg       0.71      0.76      0.73       234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = final_model.predict(X_test)\n",
    "print(f'Final results using the \\'{selected_model}\\' model:\\n')\n",
    "print(classification_report(y_test, y_pred, labels=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probability vector for predictions\n",
    "y_pred_prob = final_model.predict_proba(X_test)\n",
    "\n",
    "# Index of topic classes in our model\n",
    "model_classes = final_model.classes_.tolist()\n",
    "\n",
    "# List of correct article ids for each topic\n",
    "topic_articles = df_test.groupby('topic')['article_number'].apply(lambda x: x.values.tolist()).to_dict()\n",
    "\n",
    "# Count of correct articles for each topic\n",
    "topic_counts = {x: len(topic_articles[x]) for x in topic_articles}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Topic name                                 Suggested articles Precision Recall   F1\n",
      "       ARTS CULTURE ENTERTAINMENT            9952,9703,9789,9830,9604,9933,9526,9834      0.38   1.00 0.55\n",
      " BIOGRAPHIES PERSONALITIES PEOPLE       9896,9878,9940,9695,9758,9988,9783,9854,9645      0.78   0.47 0.58\n",
      "                          DEFENCE  9559,9576,9616,9607,9670,9721,9731,9713,9706,9739      0.80   0.62 0.70\n",
      "                 DOMESTIC MARKETS                           9994,9796,9989,9640,9923      0.40   1.00 0.57\n",
      "                    FOREX MARKETS  9551,9529,9530,9548,9555,9565,9525,9503,9539,9554      0.70   0.15 0.24\n",
      "                           HEALTH  9873,9661,9807,9810,9887,9735,9609,9833,9621,9575      0.80   0.57 0.67\n",
      "                    MONEY MARKETS  9516,9534,9509,9531,9547,9550,9560,9506,9564,9542      0.60   0.09 0.15\n",
      "           SCIENCE AND TECHNOLOGY                                     9617,9982,9722      0.33   0.33 0.33\n",
      "                   SHARE LISTINGS  9518,9666,9601,9715,9972,9562,9667,9655,9688,9999      0.60   0.86 0.71\n",
      "                           SPORTS  9574,9569,9568,9573,9513,9536,9541,9514,9520,9508      0.90   0.15 0.26\n"
     ]
    }
   ],
   "source": [
    "recommendations = {label: [] for label in sorted(list(set(y_test)))}\n",
    "\n",
    "# Loop through articles\n",
    "for index, article in enumerate(df_test.article_number.values):\n",
    "    # The topic we predicted\n",
    "    pred_class = y_pred[index]\n",
    "    # The topic it belongs to\n",
    "    true_class = y_test[index]\n",
    "    # Index of predicted class\n",
    "    pred_index = model_classes.index(pred_class)\n",
    "    # Probability of prediction\n",
    "    pred_prob = y_pred_prob[index][pred_index]\n",
    "    # Save recommendation\n",
    "    recommendations[pred_class].append((article, pred_prob))\n",
    "\n",
    "\n",
    "results = []\n",
    "    \n",
    "for topic in recommendations:\n",
    "    if topic == 'IRRELEVANT':\n",
    "        continue\n",
    "    top_10 = sorted(recommendations[topic][:10], key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    articles = [int(x[0]) for x in top_10]\n",
    "    \n",
    "    tp = len(set(articles).intersection(topic_articles[topic]))\n",
    "    precision = tp / len(articles) if len(articles) > 0 else 0\n",
    "    recall = tp / topic_counts[topic] if topic_counts[topic] > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    results.append({\n",
    "        'Topic name': topic,\n",
    "        'Suggested articles': ','.join([str(x) for x in articles]),\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1\n",
    "    })\n",
    "\n",
    "df_recommend = pd.DataFrame(results, columns=['Topic name', 'Suggested articles', 'Precision', 'Recall', 'F1'])\n",
    "print(df_recommend.to_string(index=False,formatters={'Precision':'{:,.2f}'.format, 'Recall':'{:,.2f}'.format, 'F1':'{:,.2f}'.format}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
