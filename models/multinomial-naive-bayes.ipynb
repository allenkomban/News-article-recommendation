{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, make_scorer\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multi_NB'\n",
    "\n",
    "df_train = pd.read_csv('../data/training.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9500,) (9500,) (500,) (500,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = df_train['article_words'].values\n",
    "y_train = df_train['topic'].values\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "labels = set(y_train)\n",
    "# labels.remove('IRRELEVANT')\n",
    "custom_f1 = make_scorer(f1_score, labels=list(labels), average='macro')\n",
    "\n",
    "scorer = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, labels=list(labels), average='micro'),\n",
    "    'recall': make_scorer(recall_score, labels=list(labels), average='micro'),\n",
    "    'f1': make_scorer(f1_score, labels=list(labels), average='micro'),    \n",
    "    'precision_macro': make_scorer(precision_score, labels=list(labels), average='macro'),\n",
    "    'recall_macro': make_scorer(recall_score, labels=list(labels), average='macro'),\n",
    "    'f1_macro': make_scorer(f1_score, labels=list(labels), average='macro'),\n",
    "    'custom': custom_f1,\n",
    "    \n",
    "}\n",
    "\n",
    "clf = make_pipeline_imb(\n",
    "    TfidfVectorizer(),\n",
    "    SMOTE(random_state=0,n_jobs=-1),\n",
    "    MultinomialNB(),\n",
    ")\n",
    "\n",
    "grid_params = {\n",
    "#     'multinomialnb__alpha': [0.1, 0.5, 1.0],\n",
    "    'multinomialnb__alpha': np.linspace(0.5, 1.5, 5),\n",
    "    'multinomialnb__fit_prior': [True, False], \n",
    "    'tfidfvectorizer__max_features': [20_000],\n",
    "    'tfidfvectorizer__max_df': [0.5],\n",
    "    'tfidfvectorizer__sublinear_tf': [True],\n",
    "    'tfidfvectorizer__lowercase': [False],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.58\n",
      "Best params: {'multinomialnb__alpha': 0.5, 'multinomialnb__fit_prior': True, 'tfidfvectorizer__lowercase': False, 'tfidfvectorizer__max_df': 0.5, 'tfidfvectorizer__max_features': 20000, 'tfidfvectorizer__sublinear_tf': True}\n",
      "CPU times: user 2.94 s, sys: 759 ms, total: 3.7 s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid = GridSearchCV(clf,\n",
    "                    grid_params,\n",
    "                    cv=5,\n",
    "                    n_jobs=-1,\n",
    "                    scoring=scorer,\n",
    "                    refit='custom',\n",
    "                    verbose=1,\n",
    "                    return_train_score=True)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best cross-validation score: {grid.best_score_:.2f}')\n",
    "print(f'Best params: {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidfvectorizer',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=False, max_df=0.5,\n",
       "                                 max_features=20000, min_df=1,\n",
       "                                 ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=True,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('smote',\n",
       "                 SMOTE(k_neighbors=5, n_jobs=-1, random_state=0,\n",
       "                       sampling_strategy='auto')),\n",
       "                ('multinomialnb',\n",
       "                 MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf = grid.best_estimator_\n",
    "best_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mean_train_accuracy</th>\n",
       "      <th>std_train_accuracy</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>mean_train_precision</th>\n",
       "      <th>std_train_precision</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>mean_train_recall</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_recall_macro</th>\n",
       "      <th>std_test_recall_macro</th>\n",
       "      <th>mean_train_f1_macro</th>\n",
       "      <th>std_train_f1_macro</th>\n",
       "      <th>mean_test_f1_macro</th>\n",
       "      <th>std_test_f1_macro</th>\n",
       "      <th>mean_train_custom</th>\n",
       "      <th>std_train_custom</th>\n",
       "      <th>mean_test_custom</th>\n",
       "      <th>std_test_custom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>multi_NB</td>\n",
       "      <td>0.756316</td>\n",
       "      <td>0.003184</td>\n",
       "      <td>0.666737</td>\n",
       "      <td>0.010077</td>\n",
       "      <td>0.756316</td>\n",
       "      <td>0.003184</td>\n",
       "      <td>0.666737</td>\n",
       "      <td>0.010077</td>\n",
       "      <td>0.756316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.710465</td>\n",
       "      <td>0.016987</td>\n",
       "      <td>0.753196</td>\n",
       "      <td>0.003805</td>\n",
       "      <td>0.584823</td>\n",
       "      <td>0.015956</td>\n",
       "      <td>0.753196</td>\n",
       "      <td>0.003805</td>\n",
       "      <td>0.584823</td>\n",
       "      <td>0.015956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  mean_train_accuracy  std_train_accuracy  mean_test_accuracy  \\\n",
       "0  multi_NB             0.756316            0.003184            0.666737   \n",
       "\n",
       "   std_test_accuracy  mean_train_precision  std_train_precision  \\\n",
       "0           0.010077              0.756316             0.003184   \n",
       "\n",
       "   mean_test_precision  std_test_precision  mean_train_recall  ...  \\\n",
       "0             0.666737            0.010077           0.756316  ...   \n",
       "\n",
       "   mean_test_recall_macro  std_test_recall_macro  mean_train_f1_macro  \\\n",
       "0                0.710465               0.016987             0.753196   \n",
       "\n",
       "   std_train_f1_macro  mean_test_f1_macro  std_test_f1_macro  \\\n",
       "0            0.003805            0.584823           0.015956   \n",
       "\n",
       "   mean_train_custom  std_train_custom  mean_test_custom  std_test_custom  \n",
       "0           0.753196          0.003805          0.584823         0.015956  \n",
       "\n",
       "[1 rows x 33 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_idx = grid.cv_results_['params'].index(grid.best_params_)\n",
    "\n",
    "d = {\n",
    "    'model': model_name,\n",
    "}\n",
    "\n",
    "for metric in scorer:\n",
    "    for dataset in ['train', 'test']:\n",
    "        for stat in ['mean', 'std']:\n",
    "            metric_item = f'{stat}_{dataset}_{metric}'\n",
    "            d[metric_item] = grid.cv_results_[metric_item][best_idx]\n",
    "\n",
    "df_models_clf = pd.DataFrame(d, index=[0])\n",
    "\n",
    "with open(f'../data/pickles/best_{model_name}.pickle', 'wb') as output:\n",
    "    pickle.dump(best_clf, output)\n",
    "    \n",
    "with open(f'../data/pickles/df_results_{model_name}.pickle', 'wb') as output:\n",
    "    pickle.dump(df_models_clf, output)\n",
    "    \n",
    "df_models_clf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
