{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22.2.post1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from collections import Counter\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the datasets\n",
    "df_train = pd.read_csv('../data/training.csv', index_col=0)\n",
    "\n",
    "X_train = df_train['article_words'].values\n",
    "y_train = df_train['topic'].values\n",
    "\n",
    "assert(len(X_train) == 9500)\n",
    "assert(len(y_train) == 9500)\n",
    "\n",
    "labels = set(y_train)\n",
    "labels.remove('IRRELEVANT')\n",
    "custom_f1 = make_scorer(f1_score, labels=list(labels), average='macro')\n",
    "\n",
    "scorer = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, labels=list(labels), average='micro'),\n",
    "    'recall': make_scorer(recall_score, labels=list(labels), average='micro'),\n",
    "    'f1': make_scorer(f1_score, labels=list(labels), average='micro'),    \n",
    "    'precision_macro': make_scorer(precision_score, labels=list(labels), average='macro'),\n",
    "    'recall_macro': make_scorer(recall_score, labels=list(labels), average='macro'),\n",
    "    'f1_macro': make_scorer(f1_score, labels=list(labels), average='macro'),\n",
    "    'custom': custom_f1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting up pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   33.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.55\n",
      "Best params: {'bernoullinb__alpha': 2.0, 'tfidfvectorizer__lowercase': False, 'tfidfvectorizer__max_df': 0.5, 'tfidfvectorizer__max_features': 20000, 'tfidfvectorizer__sublinear_tf': True}\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline_imb(\n",
    "    TfidfVectorizer(stop_words='english'),\n",
    "    #SMOTE(random_state=0,n_jobs=-1),\n",
    "    RandomOverSampler(sampling_strategy={'IRRELEVANT': 8000, 'MONEY MARKETS': 1673, 'SPORTS': 1673,\n",
    "                                              'FOREX MARKETS': 1500, 'DEFENCE': 1500, 'SHARE LISTINGS': 1500, 'HEALTH': 1500,\n",
    "                                              'BIOGRAPHIES PERSONALITIES PEOPLE': 1673, 'DOMESTIC MARKETS': 1500, 'ARTS CULTURE ENTERTAINMENT': 1500,\n",
    "                                              'SCIENCE AND TECHNOLOGY': 1500}),\n",
    "    BernoulliNB(),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "grid_params = {\n",
    "    'bernoullinb__alpha': [0.01,0.1,0.5,1.0, 2.0],\n",
    "\n",
    "    'tfidfvectorizer__max_features': [20_000],\n",
    "    'tfidfvectorizer__max_df': [0.5],\n",
    "    'tfidfvectorizer__sublinear_tf': [True],\n",
    "    'tfidfvectorizer__lowercase': [False],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "grid = GridSearchCV(clf,\n",
    "                    grid_params,\n",
    "                    cv=5,\n",
    "                    n_jobs=-1,\n",
    "                    scoring=scorer,\n",
    "                    refit='custom',\n",
    "                    verbose=1,\n",
    "                    return_train_score=True)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best cross-validation score: {grid.best_score_:.2f}')\n",
    "print(f'Best params: {grid.best_params_}')\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### refitting the best parameter by grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.55\n",
      "Best params: {'bernoullinb__alpha': 2.0, 'tfidfvectorizer__lowercase': False, 'tfidfvectorizer__max_df': 0.5, 'tfidfvectorizer__max_features': 20000, 'tfidfvectorizer__sublinear_tf': True}\n"
     ]
    }
   ],
   "source": [
    "print(f'Best cross-validation score: {grid.best_score_:.2f}')\n",
    "print(f'Best params: {grid.best_params_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving the results to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mean_train_accuracy</th>\n",
       "      <th>std_train_accuracy</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>mean_train_precision</th>\n",
       "      <th>std_train_precision</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>mean_train_recall</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_recall_macro</th>\n",
       "      <th>std_test_recall_macro</th>\n",
       "      <th>mean_train_f1_macro</th>\n",
       "      <th>std_train_f1_macro</th>\n",
       "      <th>mean_test_f1_macro</th>\n",
       "      <th>std_test_f1_macro</th>\n",
       "      <th>mean_train_custom</th>\n",
       "      <th>std_train_custom</th>\n",
       "      <th>mean_test_custom</th>\n",
       "      <th>std_test_custom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BNB</td>\n",
       "      <td>0.771842</td>\n",
       "      <td>0.002917</td>\n",
       "      <td>0.709053</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.671089</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.604036</td>\n",
       "      <td>0.006571</td>\n",
       "      <td>0.759914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.564236</td>\n",
       "      <td>0.022818</td>\n",
       "      <td>0.758379</td>\n",
       "      <td>0.002977</td>\n",
       "      <td>0.547403</td>\n",
       "      <td>0.021601</td>\n",
       "      <td>0.758379</td>\n",
       "      <td>0.002977</td>\n",
       "      <td>0.547403</td>\n",
       "      <td>0.021601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  model  mean_train_accuracy  std_train_accuracy  mean_test_accuracy  \\\n",
       "0   BNB             0.771842            0.002917            0.709053   \n",
       "\n",
       "   std_test_accuracy  mean_train_precision  std_train_precision  \\\n",
       "0           0.002036              0.671089             0.003097   \n",
       "\n",
       "   mean_test_precision  std_test_precision  mean_train_recall  ...  \\\n",
       "0             0.604036            0.006571           0.759914  ...   \n",
       "\n",
       "   mean_test_recall_macro  std_test_recall_macro  mean_train_f1_macro  \\\n",
       "0                0.564236               0.022818             0.758379   \n",
       "\n",
       "   std_train_f1_macro  mean_test_f1_macro  std_test_f1_macro  \\\n",
       "0            0.002977            0.547403           0.021601   \n",
       "\n",
       "   mean_train_custom  std_train_custom  mean_test_custom  std_test_custom  \n",
       "0           0.758379          0.002977          0.547403         0.021601  \n",
       "\n",
       "[1 rows x 33 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'BNB'\n",
    "\n",
    "best_bnb = grid.cv_results_['params'].index(grid.best_params_)\n",
    "\n",
    "d = {\n",
    "    'model': model_name,\n",
    "}\n",
    "\n",
    "for metric in scorer:\n",
    "    for dataset in ['train', 'test']:\n",
    "        for stat in ['mean', 'std']:\n",
    "            metric_item = f'{stat}_{dataset}_{metric}'\n",
    "            d[metric_item] = grid.cv_results_[metric_item][best_bnb]\n",
    "\n",
    "df_models_clf = pd.DataFrame(d, index=[0])\n",
    "\n",
    "with open(f'../data/pickles/best_{model_name}.pickle',\n",
    "          'wb') as output:\n",
    "    pickle.dump(best_bnb, output)\n",
    "\n",
    "with open(f'../data/pickles/df_results_{model_name}.pickle',\n",
    "          'wb') as output:\n",
    "    pickle.dump(df_models_clf, output)\n",
    "\n",
    "df_models_clf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
