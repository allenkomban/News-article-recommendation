{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression\n",
    "\n",
    "https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, make_scorer\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the model name\n",
    "\n",
    "We'll use this name as an identifier for referencing files and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multi_lr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9500,) (9500,)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('../data/training.csv', index_col=0)\n",
    "X_train = df_train['article_words'].values\n",
    "y_train = df_train['topic'].values\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the scoring\n",
    "\n",
    "These metrics will be collected during cross-validation for the training/validation splits. We will use these metrics for model comparison and evaluation. The evaluation metric used for model selection is **macro F1 excluding the 'irrelevant' class**. Using this metric avoids the majority class skewing our model selection while weighting fairly evenly across the remaining relevant classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = set(y_train)\n",
    "labels.remove('IRRELEVANT')\n",
    "custom_f1 = make_scorer(f1_score, labels=list(labels), average='macro')\n",
    "\n",
    "scorer = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, labels=list(labels), average='micro'),\n",
    "    'recall': make_scorer(recall_score, labels=list(labels), average='micro'),\n",
    "    'f1': make_scorer(f1_score, labels=list(labels), average='micro'),    \n",
    "    'precision_macro': make_scorer(precision_score, labels=list(labels), average='macro'),\n",
    "    'recall_macro': make_scorer(recall_score, labels=list(labels), average='macro'),\n",
    "    'f1_macro': make_scorer(f1_score, labels=list(labels), average='macro'),\n",
    "    'custom': custom_f1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline_imb(\n",
    "    TfidfVectorizer(),\n",
    "    SMOTE(random_state=0,n_jobs=-1),\n",
    "    LogisticRegression(random_state=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review configurable parameters\n",
    "\n",
    "For each step in the pipeline, research what each parameter does and decide on a range to test in our grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidfvectorizer\n",
      "{'analyzer': 'word',\n",
      " 'binary': False,\n",
      " 'decode_error': 'strict',\n",
      " 'dtype': <class 'numpy.float64'>,\n",
      " 'encoding': 'utf-8',\n",
      " 'input': 'content',\n",
      " 'lowercase': True,\n",
      " 'max_df': 1.0,\n",
      " 'max_features': None,\n",
      " 'min_df': 1,\n",
      " 'ngram_range': (1, 1),\n",
      " 'norm': 'l2',\n",
      " 'preprocessor': None,\n",
      " 'smooth_idf': True,\n",
      " 'stop_words': None,\n",
      " 'strip_accents': None,\n",
      " 'sublinear_tf': False,\n",
      " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      " 'tokenizer': None,\n",
      " 'use_idf': True,\n",
      " 'vocabulary': None}\n",
      "smote\n",
      "{'k_neighbors': 5, 'n_jobs': -1, 'random_state': 0, 'sampling_strategy': 'auto'}\n",
      "logisticregression\n",
      "{'C': 1.0,\n",
      " 'class_weight': None,\n",
      " 'dual': False,\n",
      " 'fit_intercept': True,\n",
      " 'intercept_scaling': 1,\n",
      " 'l1_ratio': None,\n",
      " 'max_iter': 100,\n",
      " 'multi_class': 'auto',\n",
      " 'n_jobs': None,\n",
      " 'penalty': 'l2',\n",
      " 'random_state': 0,\n",
      " 'solver': 'lbfgs',\n",
      " 'tol': 0.0001,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "for step in clf.steps:\n",
    "    print(step[0])\n",
    "    pprint(step[1].get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select grid parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params = {\n",
    "    'logisticregression__C': [0.1, 0.5, 1.0],\n",
    "    'logisticregression__solver': ['newton-cg', 'lbfgs', 'saga'],\n",
    "    'tfidfvectorizer__max_features': [20_000],\n",
    "    'tfidfvectorizer__max_df': [0.5],\n",
    "    'tfidfvectorizer__sublinear_tf': [True],\n",
    "    'tfidfvectorizer__lowercase': [False],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid = GridSearchCV(clf,\n",
    "                    grid_params,\n",
    "                    cv=5,\n",
    "                    n_jobs=-1,\n",
    "                    scoring=scorer,\n",
    "                    refit='custom',\n",
    "                    verbose=1,\n",
    "                    return_train_score=True)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best cross-validation score: {grid.best_score_:.2f}')\n",
    "print(f'Best params: {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refit the best model \n",
    "\n",
    "Here we take the best model found during cross-validation and fit it with the entire training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidfvectorizer',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=False, max_df=0.5,\n",
       "                                 max_features=20000, min_df=1,\n",
       "                                 ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=True,\n",
       "                                 toke...\n",
       "                ('smote',\n",
       "                 SMOTE(k_neighbors=5, n_jobs=-1, random_state=0,\n",
       "                       sampling_strategy='auto')),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=0,\n",
       "                                    solver='newton-cg', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf = grid.best_estimator_\n",
    "best_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model and cross-validation statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mean_train_accuracy</th>\n",
       "      <th>std_train_accuracy</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>mean_train_precision</th>\n",
       "      <th>std_train_precision</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>mean_train_recall</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_recall_macro</th>\n",
       "      <th>std_test_recall_macro</th>\n",
       "      <th>mean_train_f1_macro</th>\n",
       "      <th>std_train_f1_macro</th>\n",
       "      <th>mean_test_f1_macro</th>\n",
       "      <th>std_test_f1_macro</th>\n",
       "      <th>mean_train_custom</th>\n",
       "      <th>std_train_custom</th>\n",
       "      <th>mean_test_custom</th>\n",
       "      <th>std_test_custom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multi_lr</td>\n",
       "      <td>0.897237</td>\n",
       "      <td>0.001914</td>\n",
       "      <td>0.772842</td>\n",
       "      <td>0.006188</td>\n",
       "      <td>0.824478</td>\n",
       "      <td>0.002984</td>\n",
       "      <td>0.655177</td>\n",
       "      <td>0.008556</td>\n",
       "      <td>0.921737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.70917</td>\n",
       "      <td>0.017699</td>\n",
       "      <td>0.890682</td>\n",
       "      <td>0.002822</td>\n",
       "      <td>0.655718</td>\n",
       "      <td>0.018572</td>\n",
       "      <td>0.890682</td>\n",
       "      <td>0.002822</td>\n",
       "      <td>0.655718</td>\n",
       "      <td>0.018572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  mean_train_accuracy  std_train_accuracy  mean_test_accuracy  \\\n",
       "0  multi_lr             0.897237            0.001914            0.772842   \n",
       "\n",
       "   std_test_accuracy  mean_train_precision  std_train_precision  \\\n",
       "0           0.006188              0.824478             0.002984   \n",
       "\n",
       "   mean_test_precision  std_test_precision  mean_train_recall  ...  \\\n",
       "0             0.655177            0.008556           0.921737  ...   \n",
       "\n",
       "   mean_test_recall_macro  std_test_recall_macro  mean_train_f1_macro  \\\n",
       "0                 0.70917               0.017699             0.890682   \n",
       "\n",
       "   std_train_f1_macro  mean_test_f1_macro  std_test_f1_macro  \\\n",
       "0            0.002822            0.655718           0.018572   \n",
       "\n",
       "   mean_train_custom  std_train_custom  mean_test_custom  std_test_custom  \n",
       "0           0.890682          0.002822          0.655718         0.018572  \n",
       "\n",
       "[1 rows x 33 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_idx = grid.cv_results_['params'].index(grid.best_params_)\n",
    "\n",
    "d = {\n",
    "    'model': model_name,\n",
    "}\n",
    "\n",
    "for metric in scorer:\n",
    "    for dataset in ['train', 'test']:\n",
    "        for stat in ['mean', 'std']:\n",
    "            metric_item = f'{stat}_{dataset}_{metric}'\n",
    "            d[metric_item] = grid.cv_results_[metric_item][best_idx]\n",
    "\n",
    "df_models_clf = pd.DataFrame(d, index=[0])\n",
    "\n",
    "with open(f'../data/pickles/best_{model_name}.pickle', 'wb') as output:\n",
    "    pickle.dump(best_clf, output)\n",
    "    \n",
    "with open(f'../data/pickles/df_results_{model_name}.pickle', 'wb') as output:\n",
    "    pickle.dump(df_models_clf, output)\n",
    "    \n",
    "df_models_clf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
